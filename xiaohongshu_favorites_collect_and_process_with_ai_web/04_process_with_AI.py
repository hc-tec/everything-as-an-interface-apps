import asyncio
import json
import re
from typing import Any, Dict, List, Optional

from client_sdk.params import TaskParams
from client_sdk.rpc_client import EAIRPCClient  # type: ignore
from utils.file_utils import read_json_with_project_root, write_json_with_project_root, PROJECT_ROOT

# ----------------------
# Config
# ----------------------
INPUT_NORMALIZED_PATH = "data/favorite_notes_normalized.json"
OUTPUT_AI_RESULT_PATH = "data/favorite_notes_ai_processed.json"
FAIL_LOG_PATH = "data/favorite_notes_ai_failures.json"

# æ§åˆ¶æ˜¯å¦é‡å¤„ç†å·²å®Œæˆçš„ç¬”è®°ï¼Œä»¥åŠæ˜¯å¦å¯¹éƒ¨åˆ†å®Œæˆçš„ç¬”è®°ç»§ç»­è¡¥é½å‰©ä½™ä»»åŠ¡
REPROCESS_EXISTING = False
RESUME_PARTIAL = True

# RPC client config
RPC_BASE_URL = "http://127.0.0.1:8008"
RPC_API_KEY = "testkey"
RPC_WEBHOOK_HOST = "127.0.0.1"
RPC_WEBHOOK_PORT = 0

# Yuanbao chat params
COOKIE_IDS = [
    "819969a2-9e59-46f5-b0ca-df2116d9c2a0"
]
CONV_ID = "3df6b8e0-5ddc-444a-be11-a866b8342a39"

# æ¯5sè¿›å…¥ä¸‹ä¸€ä¸ªç¬”è®°
AI_INTERVAL_SEC = 5

# ----------------------
# Utilities
# ----------------------

def _clean_text(s: Optional[str]) -> str:
    if not s:
        return ""
    return s.replace("\n", "")


def _join_tags(tags: Optional[List[str]]) -> str:
    if not tags:
        return ""
    seen = set()
    cleaned: List[str] = []
    for t in tags:
        if not t:
            continue
        k = str(t).strip()
        if not k:
            continue
        low = k.lower()
        if low in seen:
            continue
        seen.add(low)
        cleaned.append(k)
    return ", ".join(cleaned)


def _json_try_loads(s: str) -> Optional[dict]:
    try:
        return json.loads(s)
    except Exception:
        return None


def _extract_json_from_text(text: str) -> Optional[dict]:
    if not text:
        return None
    # Remove code fences if any
    text2 = re.sub(r"```(json)?", "", text, flags=re.IGNORECASE).strip()
    data = _json_try_loads(text2)
    if data is not None:
        return data
    # Try to find first {...} block
    start = text2.find("{")
    end = text2.rfind("}")
    if start != -1 and end != -1 and end > start:
        chunk = text2[start : end + 1]
        return _json_try_loads(chunk)
    return None

# ----------------------
# Prompts
# ----------------------

def build_prompt_summary(title: str, desc: str, tags_joined: str) -> str:
    return (
        "ä½ æ˜¯å†…å®¹æ‘˜è¦åŠ©æ‰‹ã€‚ç»™å®šã€æ ‡é¢˜ã€‘ã€æ­£æ–‡ã€‘ã€æ ‡ç­¾ã€‘ï¼š"
        "- è‹¥æ­£æ–‡å……åˆ†ï¼Œä»¥æ­£æ–‡ä¸ºä¸»ã€‚"
        "- è‹¥æ­£æ–‡ç¨€ç¼ºï¼Œä»…åŸºäºæ ‡é¢˜ä¸æ ‡ç­¾ï¼›ç¦æ­¢è™šæ„ç»†èŠ‚ã€‚"
        "- æ‘˜è¦â‰¤200å­—ã€‚è¾“å‡ºJSONï¼š{\"summary_200\":\"...\", \"confidence\":0~1}"
        f"ã€æ ‡é¢˜ã€‘{title}"
        f"ã€æ­£æ–‡ã€‘{desc}"
        f"ã€æ ‡ç­¾ã€‘{tags_joined}"
    )


def build_prompt_keywords(title: str, content_for_keywords: str) -> str:
    return (
        "åŸºäºç»™å®šå†…å®¹ï¼Œæå–3-5ä¸ªåè¯ç±»å…³é”®è¯ï¼ˆä¸“æœ‰åè¯ä¼˜å…ˆï¼‰ã€‚è¾“å‡ºJSONï¼š"
        '{"keywords":["...","..."], "confidence":0~1}'
        "å†…å®¹ï¼š"
        f"{title}{content_for_keywords}"
    )


def build_prompt_topics(title: str, desc: str, tags_joined: str) -> str:
    return (
        "ä»å†…å®¹ä¸­åˆ¤å®š primary_topicã€subtopicsã€content_intentã€content_typeã€‚"
        "å¿…é¡»ä»ç»™å®šè¯è¡¨ä¸­é€‰æ‹©ï¼Œè¾“å‡ºJSONï¼ˆå¤šä½™å­—æ®µä¸è¦ï¼‰ï¼š"
        '{"primary_topic":"â€¦","subtopics":["â€¦"],"content_intent":"â€¦","content_type":"â€¦","confidence":0~1}'
        "è¯è¡¨ï¼š"
        "primary_topic: [AIå·¥å…·, ç©¿æ­, æ—…è¡Œ, å¥èº«, ç†è´¢, æ‘„å½±, ç¾é£Ÿ, æ•™è‚², èŒåœº, å¿ƒç†, å®¶å±…, äº²å­, å® ç‰©, å½±è§†, æ¸¸æˆ, ç§‘æŠ€]"
        "content_intent: [æ•™ç¨‹, ç»éªŒåˆ†äº«, æµ‹è¯„, ç§è‰, è®°å½•, æ–°é—», æ´»åŠ¨, æ‹›è˜, å¹¿å‘Š]"
        "content_type: [å›¾æ–‡, é•¿æ–‡, çŸ­è§†é¢‘, æ•™ç¨‹æ¸…å•, æµ‹è¯„å¯¹æ¯”, éšç¬”]"
        "å†…å®¹ï¼š"
        f"{title}{desc}{tags_joined}"
    )


# ----------------------
# Persistence helpers (immediate write, resumable)
# ----------------------

def _load_processed_state() -> Dict[str, Any]:
    try:
        state = read_json_with_project_root(OUTPUT_AI_RESULT_PATH)
        if isinstance(state, dict):
            # å…¼å®¹æ—§ç»“æ„ï¼šç¡®ä¿ data ä¸º list
            if not isinstance(state.get("data"), list):
                state["data"] = []
            return state
    except FileNotFoundError:
        pass
    return {
        "platform": None,
        "source": INPUT_NORMALIZED_PATH,
        "tasks": [],
        "count": 0,
        "data": [],
    }


def _save_processed_state(state: Dict[str, Any]) -> None:
    # é‡æ–°è®¡ç®— count
    state["count"] = len(state.get("data", []))
    write_json_with_project_root(OUTPUT_AI_RESULT_PATH, state)


def _append_failure_log(record: Dict[str, Any]) -> None:
    try:
        existing = read_json_with_project_root(FAIL_LOG_PATH)
        if not isinstance(existing, list):
            existing = []
    except FileNotFoundError:
        existing = []
    existing.append(record)
    write_json_with_project_root(FAIL_LOG_PATH, existing)


def _index_by_note_id(state: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    index: Dict[str, Dict[str, Any]] = {}
    for item in state.get("data", []):
        nid = item.get("note_id")
        if isinstance(nid, str) and nid:
            index[nid] = item
    return index


def _merge_note_result_into_state(state: Dict[str, Any], note_result: Dict[str, Any]) -> None:
    # å¦‚å­˜åœ¨ç›¸åŒ note_id åˆ™æ›¿æ¢ï¼Œå¦åˆ™è¿½åŠ 
    note_id = note_result.get("note_id")
    assert isinstance(note_id, str) and note_id
    replaced = False
    for i, item in enumerate(state.get("data", [])):
        if item.get("note_id") == note_id:
            state["data"][i] = note_result
            replaced = True
            break
    if not replaced:
        state.setdefault("data", []).append(note_result)


# ----------------------
# Task Abstractions
# ----------------------

class Task:
    name: str

    def prompt(self, normalized: dict) -> str:
        raise NotImplementedError

    def parse_and_validate(self, model_text: str) -> Dict[str, Any]:
        raise NotImplementedError

    async def run(self, client: EAIRPCClient, normalized: dict) -> Dict[str, Any]:
        p = self.prompt(normalized)
        try:
            chat_result = await client.chat_with_yuanbao(
                ask_question=p,
                conversation_id=CONV_ID,
                task_params=TaskParams(
                    cookie_ids=COOKIE_IDS,
                    close_page_when_task_finished=True,
                ),
            )
            data = chat_result.get("data") if isinstance(chat_result, dict) else None
            if not (isinstance(data, list) and data and isinstance(data[0], dict)):
                raise RuntimeError("unexpected AI response shape")
            text = data[0].get("last_model_message")
            if not isinstance(text, str) or not text.strip():
                raise RuntimeError("empty model message")
            parsed = self.parse_and_validate(text)
            return {"ok": True, "result": parsed, "raw": text}
        except Exception as e:
            err = {"type": type(e).__name__, "message": str(e)}
            # è‹¥èƒ½æ‹¿åˆ°åŸå§‹æ–‡æœ¬ï¼Œå°è¯•é™„å¸¦æˆªæ–­ä¿¡æ¯
            raw = None
            try:
                raw = locals().get("text")  # type: ignore
            except Exception:
                raw = None
            return {"ok": False, "error": err, "raw": raw, "prompt_excerpt": p}


class SummaryTask(Task):
    name = "summary"

    def prompt(self, normalized: dict) -> str:
        title = _clean_text((normalized or {}).get("title"))
        desc = _clean_text((normalized or {}).get("desc"))
        tags_joined = _join_tags((normalized or {}).get("tags") or [])
        return build_prompt_summary(title, desc, tags_joined)

    def parse_and_validate(self, model_text: str) -> Dict[str, Any]:
        data = _extract_json_from_text(model_text)
        if not isinstance(data, dict):
            raise ValueError("summary: invalid JSON")
        if "summary_200" not in data or not isinstance(data["summary_200"], str):
            raise ValueError("summary: missing summary_200")
        # confidence å¯é€‰
        return {"summary_200": data["summary_200"], "confidence": data.get("confidence")}


class KeywordsTask(Task):
    name = "keywords"

    def prompt(self, normalized: dict) -> str:
        title = _clean_text((normalized or {}).get("title"))
        desc = _clean_text((normalized or {}).get("desc"))
        tags = (normalized or {}).get("tags") or []
        # å½“æ­£æ–‡ä¸è¶³æ—¶ï¼Œå…³é”®è¯æå–å¯å¼ºè°ƒæ ‡é¢˜/æ ‡ç­¾
        content_for_keywords = desc if desc else (title + "" + _join_tags(tags))
        return build_prompt_keywords(title, content_for_keywords)

    def parse_and_validate(self, model_text: str) -> Dict[str, Any]:
        data = _extract_json_from_text(model_text)
        if not isinstance(data, dict):
            raise ValueError("keywords: invalid JSON")
        kws = data.get("keywords")
        if not isinstance(kws, list) or not all(isinstance(x, str) for x in kws):
            raise ValueError("keywords: missing or invalid keywords array")
        return {"keywords": kws, "confidence": data.get("confidence")}


class TopicsTask(Task):
    name = "topics"

    ALLOWED_PRIMARY = ["AIå·¥å…·", "ç©¿æ­", "æ—…è¡Œ", "å¥èº«", "ç†è´¢", "æ‘„å½±", "ç¾é£Ÿ", "æ•™è‚²", "èŒåœº", "å¿ƒç†", "å®¶å±…", "äº²å­", "å® ç‰©", "å½±è§†", "æ¸¸æˆ", "ç§‘æŠ€"]
    ALLOWED_INTENT = ["æ•™ç¨‹", "ç»éªŒåˆ†äº«", "æµ‹è¯„", "ç§è‰", "è®°å½•", "æ–°é—»", "æ´»åŠ¨", "æ‹›è˜", "å¹¿å‘Š"]
    ALLOWED_TYPE = ["å›¾æ–‡", "é•¿æ–‡", "çŸ­è§†é¢‘", "æ•™ç¨‹æ¸…å•", "æµ‹è¯„å¯¹æ¯”", "éšç¬”"]

    def prompt(self, normalized: dict) -> str:
        title = _clean_text((normalized or {}).get("title"))
        desc = _clean_text((normalized or {}).get("desc"))
        tags_joined = _join_tags((normalized or {}).get("tags") or [])
        return build_prompt_topics(title, desc, tags_joined)

    def parse_and_validate(self, model_text: str) -> Dict[str, Any]:
        data = _extract_json_from_text(model_text)
        if not isinstance(data, dict):
            raise ValueError("topics: invalid JSON")
        required = ["primary_topic", "subtopics", "content_intent", "content_type"]
        for k in required:
            if k not in data:
                raise ValueError(f"topics: missing {k}")
        if data["primary_topic"] not in self.ALLOWED_PRIMARY:
            raise ValueError("topics: primary_topic not in allowed list")
        if not isinstance(data["subtopics"], list) or not all(isinstance(x, str) for x in data["subtopics"]):
            raise ValueError("topics: subtopics must be list[str]")
        if data["content_intent"] not in self.ALLOWED_INTENT:
            raise ValueError("topics: content_intent not in allowed list")
        if data["content_type"] not in self.ALLOWED_TYPE:
            raise ValueError("topics: content_type not in allowed list")
        return {
            "primary_topic": data["primary_topic"],
            "subtopics": data["subtopics"],
            "content_intent": data["content_intent"],
            "content_type": data["content_type"],
            "confidence": data.get("confidence"),
        }


# Registry of tasks (extensible)
TASKS: List[Task] = [SummaryTask(), KeywordsTask(), TopicsTask()]


# ----------------------
# Per-note processing and immediate persistence
# ----------------------

async def process_one_note(client: EAIRPCClient, normalized_item: dict, existing: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    norm = normalized_item.get("normalized", {}) if isinstance(normalized_item, dict) else {}
    note_id = norm.get("note_id") or normalized_item.get("id") or ""

    # åˆå§‹åŒ–/æ‰¿æ¥å·²å­˜åœ¨çš„ç»“æœï¼ˆç”¨äºæ–­ç‚¹ç»­è·‘ï¼Œä»…è¡¥æœªå®Œæˆä»»åŠ¡ï¼‰
    note_result: Dict[str, Any] = existing.copy() if isinstance(existing, dict) else {"note_id": note_id, "tasks": {}}

    # é€ä»»åŠ¡æ‰§è¡Œï¼›è‹¥å·²æœ‰è¯¥ä»»åŠ¡ä¸” ok ä¸”ä¸é‡è·‘ï¼Œåˆ™è·³è¿‡
    for task in TASKS:
        task_state = (note_result.get("tasks") or {}).get(task.name)
        if task_state and task_state.get("ok") and not REPROCESS_EXISTING:
            continue
        # è¿è¡Œä»»åŠ¡
        res = await task.run(client, norm)
        # å†™å…¥ä»»åŠ¡ç»“æœ
        note_result.setdefault("tasks", {})[task.name] = res
        # å¤±è´¥åˆ™å†™ä¸€æ¡å¤±è´¥æ—¥å¿—ï¼ˆä½†ä¸ä¸­æ–­å…¶ä»–ä»»åŠ¡ï¼‰
        if not res.get("ok"):
            _append_failure_log({
                "note_id": note_id,
                "task": task.name,
                "error": res.get("error"),
                "raw_response_excerpt": res.get("raw"),
                "prompt_excerpt": res.get("prompt_excerpt"),
            })

    # æ±‡æ€»çŠ¶æ€
    task_values = list((note_result.get("tasks") or {}).values())
    oks = [t for t in task_values if t.get("ok")]
    if len(oks) == len(task_values) and task_values:
        status = "ok"
    elif any(t.get("ok") for t in task_values):
        status = "partial"
    else:
        status = "failed"
    note_result["status"] = status

    # å…¼å®¹ï¼šæ‹å¹³æˆåŠŸä»»åŠ¡ï¼ˆsummary/keywords/topicsï¼‰ä»¥å…¼å®¹åç»­å¯èƒ½çš„æ¶ˆè´¹ç«¯
    if note_result["tasks"].get("summary", {}).get("ok"):
        note_result["summary"] = note_result["tasks"]["summary"]["result"]
    if note_result["tasks"].get("keywords", {}).get("ok"):
        note_result["keywords"] = note_result["tasks"]["keywords"]["result"]
    if note_result["tasks"].get("topics", {}).get("ok"):
        note_result["topics"] = note_result["tasks"]["topics"]["result"]

    return note_result


# ----------------------
# Main
# ----------------------

async def main():
    print("ğŸš€ AIå¤„ç†é˜¶æ®µå¯åŠ¨ï¼šè¯»å–è§„èŒƒåŒ–æ•°æ®ï¼Œæ‰§è¡Œä»»åŠ¡å¹¶å³æ—¶è½ç›˜ï¼ˆå¯æ¢å¤ï¼‰")

    norm_payload = read_json_with_project_root(INPUT_NORMALIZED_PATH)
    items: List[dict] = norm_payload.get("data") if isinstance(norm_payload, dict) else None
    if not isinstance(items, list):
        raise RuntimeError("è§„èŒƒåŒ–è¾“å…¥æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼šåº”åŒ…å« data æ•°ç»„")

    # è½½å…¥å½“å‰å¤„ç†çŠ¶æ€
    state = _load_processed_state()
    # é¦–æ¬¡è¿è¡Œæ—¶è¡¥å……å…ƒä¿¡æ¯
    if state.get("platform") is None:
        state["platform"] = norm_payload.get("platform", "xhs")
        state["source"] = INPUT_NORMALIZED_PATH
    state["tasks"] = [t.name for t in TASKS]

    index = _index_by_note_id(state)

    client = EAIRPCClient(
        base_url=RPC_BASE_URL,
        api_key=RPC_API_KEY,
        webhook_host=RPC_WEBHOOK_HOST,
        webhook_port=RPC_WEBHOOK_PORT,
    )

    try:
        await client.start()
        print("âœ… RPCå®¢æˆ·ç«¯å·²å¯åŠ¨")

        for idx, item in enumerate(items, start=1):
            note_id = (item.get("normalized") or {}).get("note_id") or item.get("id") or ""
            if not note_id:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆç¬”è®°ï¼ˆç¼ºå°‘ note_idï¼‰: index={idx}")
                continue

            existing = index.get(note_id)
            # åˆ¤æ–­æ˜¯å¦éœ€è¦å¤„ç†ï¼ˆå·²å®Œæˆä¸”ä¸é‡è·‘ -> è·³è¿‡ï¼›éƒ¨åˆ†å®Œæˆä¸”å…è®¸ç»­è·‘ -> å¤„ç†å‰©ä½™ï¼‰
            if existing:
                if existing.get("status") == "ok" and not REPROCESS_EXISTING:
                    print(f"â­ï¸ è·³è¿‡å·²å®Œæˆç¬”è®°: {note_id}")
                    continue
                if existing.get("status") == "partial" and not RESUME_PARTIAL and not REPROCESS_EXISTING:
                    print(f"â­ï¸ è·³è¿‡éƒ¨åˆ†å®Œæˆç¬”è®°ï¼ˆå·²ç¦ç”¨ç»­è·‘ï¼‰: {note_id}")
                    continue

            print(f"ğŸ§© å¤„ç†ç¬¬ {idx}/{len(items)} æ¡ç¬”è®°: {note_id}")
            note_result = await process_one_note(client, item, existing)

            # åˆå¹¶å¹¶ç«‹å³å†™ç›˜ï¼ˆä¿è¯ä»»ä½•ä¸­æ–­æ—¶æœ‰æœ€æ–°è¿›åº¦ï¼‰
            _merge_note_result_into_state(state, note_result)
            _save_processed_state(state)
            # æ›´æ–°ç´¢å¼•
            index[note_id] = note_result

            await asyncio.sleep(AI_INTERVAL_SEC)


    finally:
        await client.stop()
        print("âœ… RPCå®¢æˆ·ç«¯å·²åœæ­¢")

    # ç»“æŸæ—¶å†æ¬¡ä¿å­˜ä¸€æ¬¡ç¡®ä¿ count ç­‰èšåˆå­—æ®µæ­£ç¡®
    _save_processed_state(state)
    print(f"ğŸ’¾ å·²å†™å…¥AIå¤„ç†ç»“æœ: {(PROJECT_ROOT / OUTPUT_AI_RESULT_PATH).as_posix()}")
    print(f"ğŸ§¾ å¤±è´¥æ—¥å¿—æ–‡ä»¶: {(PROJECT_ROOT / FAIL_LOG_PATH).as_posix()}")


if __name__ == "__main__":
    asyncio.run(main())