import re
import unicodedata
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from utils.file_utils import read_json_with_project_root, write_json_with_project_root, PROJECT_ROOT

# è¾“å…¥/è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹é¡¹ç›®æ ¹ç›®å½•ï¼‰
INPUT_DETAILS_PATH = "data/favorite_notes_details.json"
OUTPUT_NORMALIZED_PATH = "data/favorite_notes_normalized.json"


def to_half_width(s: str) -> str:
    """å…¨è§’è½¬åŠè§’ï¼Œå¹¶åš NFKC è§„èŒƒåŒ–ã€‚"""
    if not s:
        return ""
    # ä½¿ç”¨ unicode è§„èŒƒåŒ–å°†å®½å­—ç¬¦æŠ˜å 
    s = unicodedata.normalize("NFKC", s)
    return s


def strip_emojis(text: str) -> str:
    """å°½é‡å»æ‰ emoji ç­‰ç¬¦å·å­—ç¬¦ï¼Œä¿ç•™å¸¸è§ä¸­è‹±æ–‡ã€æ•°å­—ã€æ ‡ç‚¹ã€‚"""
    if not text:
        return ""
    # å…ˆåšå…¼å®¹æ€§è§„èŒƒåŒ–å‡å°‘å˜ä½“
    text = unicodedata.normalize("NFKC", text)
    # è¿‡æ»¤æ‰ç±»åˆ«ä¸º Soï¼ˆSymbol, otherï¼‰å’Œ Csï¼ˆSurrogateï¼‰çš„å­—ç¬¦
    return "".join(ch for ch in text if unicodedata.category(ch) not in {"So", "Cs"})


def clean_text(text: Optional[str]) -> str:
    """æ¸…æ´—æ–‡æœ¬ï¼šå» emojiã€å‹ç¼©å¤šä½™ç©ºç™½ã€å»é¦–å°¾ç©ºç™½ã€‚"""
    if not text:
        return ""
    text = strip_emojis(text)
    # å°†å„ç§ç©ºç™½æŠ˜å ä¸ºä¸€ä¸ªç©ºæ ¼
    text = re.sub(r"\s+", " ", text, flags=re.MULTILINE).strip()
    return text


def safe_int(val: Any, default: int = 0) -> int:
    try:
        if val is None:
            return default
        if isinstance(val, bool):
            return int(val)
        if isinstance(val, (int, float)):
            return int(val)
        s = str(val).strip().replace(",", "")
        # æå–å‰å¯¼æ•°å­—ï¼ˆå¦‚ "123 ä¸ª" -> 123ï¼‰
        m = re.search(r"-?\d+", s)
        return int(m.group()) if m else default
    except Exception:
        return default


def normalize_tags(tags: Optional[List[str]]) -> List[str]:
    if not tags:
        return []
    normed: List[str] = []
    seen = set()
    for t in tags:
        if t is None:
            continue
        t = to_half_width(str(t)).strip().lower()
        # å»æ‰å‰ç¼€è¯é¢˜æ ¼å¼å¦‚ #è¯é¢˜#ã€ã€ã€‘ã€[]ç­‰åŒ…è£¹ç¬¦
        t = re.sub(r"^[#\s]+|[\s#]+$", "", t)
        t = t.strip("[]ï¼ˆï¼‰()ã€ã€‘<>")
        if not t:
            continue
        if t not in seen:
            seen.add(t)
            normed.append(t)
    return normed


def pick_video_duration_sec(video: Any) -> int:
    if not isinstance(video, dict):
        return 0
    # å¸¸è§å­—æ®µåå…œåº•
    for key in [
        "duration", "duration_sec", "durationSeconds", "length", "length_sec",
        "video_duration", "videoDuration", "time", "time_sec", "duration_ms"
    ]:
        if key in video and video[key] is not None:
            v = video[key]
            # æ¯«ç§’å­—æ®µ
            if key.endswith("_ms"):
                return safe_int(v) // 1000
            return safe_int(v)
    return 0


def to_iso8601_from_epoch_ms(epoch: Any) -> Optional[str]:
    try:
        # å…¼å®¹ç§’/æ¯«ç§’
        ts = float(epoch)
        if ts > 1e12:  # ä¼¼ä¹æ˜¯æ¯«ç§’
            ts = ts / 1000.0
        dt = datetime.fromtimestamp(ts, tz=timezone.utc)
        return dt.isoformat().replace("+00:00", "Z")
    except Exception:
        return None


def parse_published_at(item: Dict[str, Any]) -> Optional[str]:
     # ä¼˜å…ˆä½¿ç”¨æŠ“å–åˆ°çš„ dateï¼ˆå¯èƒ½ä¸ºæ¯«ç§’/ç§’æ—¶é—´æˆ³ï¼Œæˆ– ISO å­—ç¬¦ä¸²ï¼‰
     if "date" in item and item["date"] is not None:
         val = item["date"]
         # å…ˆå°è¯•æŒ‰ epochï¼ˆå­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰è§£æ
         iso = to_iso8601_from_epoch_ms(val)
         if iso:
             return iso
         # å†å°è¯• ISO æ–‡æœ¬è§£æ
         if isinstance(val, str) and val:
             try:
                 dt = datetime.fromisoformat(val.replace("Z", "+00:00"))
                 if dt.tzinfo is None:
                     dt = dt.replace(tzinfo=timezone.utc)
                 return dt.isoformat().replace("+00:00", "Z")
             except Exception:
                 pass
     # å…¶æ¬¡å°è¯•ç›´æ¥è§£æ ISO å­—ç¬¦ä¸²ï¼ˆtimestamp å­—æ®µï¼‰
     ts = item.get("timestamp")
     if isinstance(ts, str) and ts:
         try:
             dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
             if dt.tzinfo is None:
                 dt = dt.replace(tzinfo=timezone.utc)
             return dt.isoformat().replace("+00:00", "Z")
         except Exception:
             pass
     return None


def compute_age_days(published_at_iso: Optional[str]) -> Optional[int]:
    if not published_at_iso:
        return None
    try:
        dt = datetime.fromisoformat(published_at_iso.replace("Z", "+00:00"))
        now = datetime.now(timezone.utc)
        delta = now - dt
        # å–éè´Ÿå¤©æ•°
        days = max(0, int(delta.total_seconds() // 86400))
        return days
    except Exception:
        return None


def detect_lang(title: str, desc: str) -> str:
    """å¯å‘å¼è¯­è¨€æ£€æµ‹ï¼šä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ / è‹±æ–‡å­—æ¯æ¯”ä¾‹"""
    text = f"{title} {desc}".strip()
    if not text:
        return "unknown"
    # ç»Ÿè®¡ä¸­è‹±å­—ç¬¦
    zh_count = sum(1 for ch in text if "\u4e00" <= ch <= "\u9fff")
    en_count = sum(1 for ch in text if ("a" <= ch <= "z") or ("A" <= ch <= "Z"))
    total = len(text)
    zh_ratio = zh_count / total
    en_ratio = en_count / total
    if zh_ratio >= 0.3:
        return "zh"
    if en_ratio >= 0.5:
        return "en"
    return "unknown"


def build_author_link(platform: str, user_id: str, user_xsec_token: str) -> Optional[str]:
    if not user_id:
        return None
    if platform == "xhs":
        return f"https://www.xiaohongshu.com/user/profile/{user_id}?xsec_token={user_xsec_token}"
    if platform == "bilibili":
        return f"https://space.bilibili.com/{user_id}"
    if platform == "zhihu":
        return f"https://www.zhihu.com/people/{user_id}"
    if platform == "weixin":
        # å…¬ä¼—å·ä½œè€…ä¸»é¡µä¸å›ºå®šï¼Œè¿™é‡Œè¿”å› None
        return None
    return None


def normalize_one(item: Dict[str, Any], platform: str = "xhs") -> Dict[str, Any]:
    note_id = str(item.get("id", "")).strip()
    note_xsec_token = item.get("xsec_token")
    raw_title = item.get("title") or ""
    raw_desc = item.get("desc") or ""
    title = clean_text(raw_title)
    # desc åŸæ ·ä¿ç•™ï¼Œä½†è®¡ç®—é•¿åº¦æ—¶åšç®€å• strip
    desc = raw_desc if isinstance(raw_desc, str) else str(raw_desc)
    desc_length = len(desc.strip())

    tags = item.get("tags") or []
    tags_norm = normalize_tags(tags)

    images = item.get("images") if isinstance(item.get("images"), list) else []
    video = item.get("video")
    has_images = bool(images)
    has_video = video is not None
    image_count = len(images) if has_images else 0
    video_duration_sec = pick_video_duration_sec(video)

    published_at = parse_published_at(item)
    age_days = compute_age_days(published_at)

    # äº’åŠ¨æ•°æ®
    stat = item.get("statistic") or {}
    like_num = safe_int(stat.get("like_num", item.get("like_num")))
    collect_num = safe_int(stat.get("collect_num", item.get("collect_num")))
    # è¯„è®ºæ•°ï¼šä¼˜å…ˆ statistic.chat_numï¼Œå…¶æ¬¡æ ¹å­—æ®µ comment_num
    comment_num = safe_int(stat.get("chat_num", item.get("comment_num")))
    engagement_score = like_num + collect_num + comment_num

    # ä½œè€…ä¿¡æ¯
    author_info = item.get("author_info") or {}
    user_id = str(author_info.get("user_id", "")).strip()
    user_xsec_token = author_info.get("xsec_token")
    username = clean_text(author_info.get("username"))
    avatar = author_info.get("avatar") or None
    author_link = build_author_link(platform, user_id, user_xsec_token)

    # è¯­è¨€ä¸è´¨é‡æ ‡è®°
    lang = detect_lang(title, desc)
    # æ˜¯å¦å­˜åœ¨è§†é¢‘å­—å¹•ï¼šè‹¥ video ä¸º dict ä¸”å­˜åœ¨éç©º subtitles å­—æ®µï¼Œåˆ™è®¤ä¸ºæœ‰å­—å¹•
    has_video_subtitles = bool(video and isinstance(video, dict) and video.get("subtitles"))
    is_content_sparse = (desc_length < 50) and (not has_video_subtitles)
    has_only_media = (desc_length == 0) and (has_images or has_video)

    normalized = {
        "platform": platform,
        "note_id": note_id,
        "note_xsec_token": note_xsec_token,
        "title": title,
        "desc": desc,
        "desc_length": desc_length,
        "tags": tags_norm,
        "media": {
            "has_images": has_images,
            "has_video": has_video,
            "image_count": image_count,
            "video_duration_sec": video_duration_sec,
        },
        "timestamps": {
            "published_at": published_at,  # ISO8601 or None
            "age_days": age_days,
        },
        "stats": {
            "like_num": like_num,
            "collect_num": collect_num,
            "comment_num": comment_num,
            "engagement_score": engagement_score,
            "engagement_rate": None,  # éœ€è¦ä½œè€…ç²‰ä¸æ•°åè®¡ç®—
        },
        "author": {
            "user_id": user_id,
            "username": username,
            "avatar": avatar,
            "author_link": author_link,
        },
        "locale": {
            "lang": lang,
        },
        "quality_flags": {
            "is_content_sparse": is_content_sparse,
            "has_only_media": has_only_media,
        },
    }
    return {"normalized": normalized}


def normalize_all() -> Dict[str, Any]:
    src = read_json_with_project_root(INPUT_DETAILS_PATH)
    # å…¼å®¹ä¸åŒç»“æ„ï¼šå¯èƒ½æ˜¯ {"data": [...]} æˆ– ç›´æ¥æ˜¯åˆ—è¡¨
    items: List[Dict[str, Any]]
    if isinstance(src, dict):
        items = src.get("data") or []
    elif isinstance(src, list):
        items = src
    else:
        items = []

    normalized_list: List[Dict[str, Any]] = []
    for it in items:
        try:
            normalized_list.append(normalize_one(it, platform="xhs"))
        except Exception as e:
            # å¿½ç•¥å•æ¡å¼‚å¸¸ï¼Œä¿è¯æ•´ä½“å¯ç”¨
            normalized_list.append({
                "error": str(e),
                "raw_id": it.get("id") if isinstance(it, dict) else None,
            })

    result = {
        "platform": "xhs",
        "count": len([d for d in normalized_list if "normalized" in d]),
        "data": normalized_list,
        "generated_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "source_file": INPUT_DETAILS_PATH,
    }
    return result


def main():
    result = normalize_all()
    write_json_with_project_root(OUTPUT_NORMALIZED_PATH, result)
    print(f"âœ… è§„èŒƒåŒ–å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶ï¼š{(PROJECT_ROOT / OUTPUT_NORMALIZED_PATH).as_posix()}ï¼Œcount={result.get('count')}")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è§„èŒƒåŒ–æ”¶è—ç¬”è®°æ•°æ® â€¦")
    main()